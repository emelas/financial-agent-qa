{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "from utils import qa_advanced_scorer, qa_basic_scorer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "data = pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data.loc[data['qa'].isna()]['qa_0'].iloc[0])\n",
    "# display(data.loc[data['qa'].isna()]['qa_1'].iloc[0])\n",
    "# display(data.loc[data['qa'].isna()]['annotation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data.head(1)['qa'][0]['question'])\n",
    "# display(data.head(1)['annotation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_source_text(x):\n",
    "    pre_text = '\\n'.join(x['pre_text'])\n",
    "    table = x['table']\n",
    "    post_text = '\\n'.join(x['post_text'])\n",
    "    return f\"{pre_text}\\n\\ntable:\\n\\n{table}\\n\\n{post_text}\"\n",
    "\n",
    "def split_qa(x):\n",
    "    qa = x['qa']\n",
    "    qa_0 = x['qa_0']\n",
    "    qa_1 = x['qa_1']\n",
    "\n",
    "    qa_list = []\n",
    "\n",
    "    if str(qa) != 'nan':\n",
    "        qa_list.append(qa)\n",
    "    else:\n",
    "        if str(qa_0) != 'nan':\n",
    "            qa_list.append(qa_0)\n",
    "        if str(qa_1) != 'nan':\n",
    "            qa_list.append(qa_1)\n",
    "    \n",
    "    return qa_list\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_json('train.json')\n",
    "    data['source_text'] = data.apply(get_source_text, axis=1)\n",
    "    data['qa_exploded'] = data.apply(split_qa, axis=1)\n",
    "    data = data.explode('qa_exploded', ignore_index=True)\n",
    "    data['question'] = data['qa_exploded'].apply(lambda x: x['question'] if str(x)!='nan' else None)\n",
    "    data['answer'] = data['qa_exploded'].apply(lambda x: x['answer'] if str(x)!='nan' else None)\n",
    "    return data\n",
    "\n",
    "data_clean = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3965, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>post_text</th>\n",
       "      <th>filename</th>\n",
       "      <th>table_ori</th>\n",
       "      <th>table</th>\n",
       "      <th>qa</th>\n",
       "      <th>id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>qa_0</th>\n",
       "      <th>qa_1</th>\n",
       "      <th>source_text</th>\n",
       "      <th>qa_exploded</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[26 | 2009 annual report in fiscal 2008 , reve...</td>\n",
       "      <td>[year ended june 30 , cash provided by operati...</td>\n",
       "      <td>JKHY/2009/page_28.pdf</td>\n",
       "      <td>[[, Year ended June 30, 2009], [2008, 2007], [...</td>\n",
       "      <td>[[2008, year ended june 30 2009 2008, year end...</td>\n",
       "      <td>{'question': 'what was the percentage change i...</td>\n",
       "      <td>Single_JKHY/2009/page_28.pdf-3</td>\n",
       "      <td>{'amt_table': '&lt;table class='wikitable'&gt;&lt;tr&gt;&lt;t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 | 2009 annual report in fiscal 2008 , reven...</td>\n",
       "      <td>{'question': 'what was the percentage change i...</td>\n",
       "      <td>what was the percentage change in the net cash...</td>\n",
       "      <td>14.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[substantially all of the goodwill and other i...</td>\n",
       "      <td>[the above unaudited pro forma financial infor...</td>\n",
       "      <td>RSG/2008/page_114.pdf</td>\n",
       "      <td>[[, Year Ended December 31, 2008 (Unaudited), ...</td>\n",
       "      <td>[[, year ended december 31 2008 ( unaudited ),...</td>\n",
       "      <td>{'question': 'what was the percent of the grow...</td>\n",
       "      <td>Single_RSG/2008/page_114.pdf-2</td>\n",
       "      <td>{'amt_table': '&lt;table class='wikitable'&gt;&lt;tr&gt;&lt;t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>substantially all of the goodwill and other in...</td>\n",
       "      <td>{'question': 'what was the percent of the grow...</td>\n",
       "      <td>what was the percent of the growth in the reve...</td>\n",
       "      <td>1.3%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0  [26 | 2009 annual report in fiscal 2008 , reve...   \n",
       "1  [substantially all of the goodwill and other i...   \n",
       "\n",
       "                                           post_text               filename  \\\n",
       "0  [year ended june 30 , cash provided by operati...  JKHY/2009/page_28.pdf   \n",
       "1  [the above unaudited pro forma financial infor...  RSG/2008/page_114.pdf   \n",
       "\n",
       "                                           table_ori  \\\n",
       "0  [[, Year ended June 30, 2009], [2008, 2007], [...   \n",
       "1  [[, Year Ended December 31, 2008 (Unaudited), ...   \n",
       "\n",
       "                                               table  \\\n",
       "0  [[2008, year ended june 30 2009 2008, year end...   \n",
       "1  [[, year ended december 31 2008 ( unaudited ),...   \n",
       "\n",
       "                                                  qa  \\\n",
       "0  {'question': 'what was the percentage change i...   \n",
       "1  {'question': 'what was the percent of the grow...   \n",
       "\n",
       "                               id  \\\n",
       "0  Single_JKHY/2009/page_28.pdf-3   \n",
       "1  Single_RSG/2008/page_114.pdf-2   \n",
       "\n",
       "                                          annotation qa_0 qa_1  \\\n",
       "0  {'amt_table': '<table class='wikitable'><tr><t...  NaN  NaN   \n",
       "1  {'amt_table': '<table class='wikitable'><tr><t...  NaN  NaN   \n",
       "\n",
       "                                         source_text  \\\n",
       "0  26 | 2009 annual report in fiscal 2008 , reven...   \n",
       "1  substantially all of the goodwill and other in...   \n",
       "\n",
       "                                         qa_exploded  \\\n",
       "0  {'question': 'what was the percentage change i...   \n",
       "1  {'question': 'what was the percent of the grow...   \n",
       "\n",
       "                                            question answer  \n",
       "0  what was the percentage change in the net cash...  14.1%  \n",
       "1  what was the percent of the growth in the reve...   1.3%  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_clean.shape)\n",
    "data_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$12.0 million'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qa_simple_rag(text,question):\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "        api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "        temperature=0.15,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        # max_retries=3,\n",
    "    )\n",
    "\n",
    "    prompt_str = \"\"\"\n",
    "    You are reading a financial document. \n",
    "    Provide an answer to the following question based on the information provided in the text.\n",
    "    Give the answer only.\n",
    "    \\n\\n{text}\\n\\n\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"text\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\n",
    "        'text': text,\n",
    "        'question':question\n",
    "    })\n",
    "\n",
    "    return response\n",
    "\n",
    "qa_simple_rag(text,question)\n",
    "# qa_simple_rag('in 2022, the company made 200 pounds after spending 100','what is the total profit made in 2022?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG With Math Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (LLMathChain is deprecated...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langchain_experimental\\utilities\\__init__.py:2: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_experimental.utilities.python import PythonREPL\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "import numexpr\n",
    "\n",
    "def qa_maths_reasoning(text,question):\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "        api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "        temperature=0.15,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        # max_retries=3,\n",
    "    )\n",
    "    problem_chain = LLMMathChain.from_llm(llm=llm)\n",
    "    math_tool = Tool.from_function(name=\"Calculator\",\n",
    "                    func=problem_chain.run,\n",
    "                    description=\"\"\"Useful for when you need to answer questions \n",
    "                    about math. This tool is only for math questions and nothing else. Only input\n",
    "                    math expressions.\"\"\")\n",
    "\n",
    "    @tool\n",
    "    def calculator(expression: str) -> str:\n",
    "        \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "        Expression should be a single line mathematical expression\n",
    "        that solves the problem.\n",
    "\n",
    "        Examples:\n",
    "            \"37593 * 67\" for \"37593 times 67\"\n",
    "            \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "        \"\"\"\n",
    "        local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "        return str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={},  # restrict access to globals\n",
    "                local_dict=local_dict,  # add common mathematical functions\n",
    "            )\n",
    "        )\n",
    "\n",
    "    word_problem_template = \"\"\"You are a reasoning agent tasked with solving \n",
    "    the user's logic-based questions. Logically arrive at the solution, and be \n",
    "    factual. In your answers, clearly detail the steps involved and give the \n",
    "    final answer. Provide the response in bullet points. \n",
    "    Question  {question}\"\"\"\n",
    "\n",
    "    math_assistant_prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                                        template=word_problem_template\n",
    "                                        )\n",
    "    word_problem_chain = LLMChain(llm=llm,\n",
    "                                prompt=math_assistant_prompt)\n",
    "    word_problem_tool = Tool.from_function(name=\"Reasoning Tool\",\n",
    "                                        func=word_problem_chain.run,\n",
    "                                        description=\"Useful for when you need to answer logic-based/reasoning questions.\",\n",
    "                                        )\n",
    "\n",
    "    python_repl = PythonREPL()\n",
    "    repl_tool = Tool(\n",
    "        name=\"python_repl\",\n",
    "        description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "        func=python_repl.run,\n",
    "    )\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=[math_tool,calculator,word_problem_tool,repl_tool],\n",
    "        # tools=[math_tool],\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=False,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    prompt_str = \"\"\"\n",
    "    You are reading a financial document. \n",
    "    Provide an answer to the following question based on the information provided in the text.\n",
    "    Give the answer only without repeating the question or statement.\n",
    "    \\n\\n{text}\\n\\n\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"text\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | agent \n",
    "\n",
    "    response = chain.invoke({\n",
    "        'text': text,\n",
    "        'question':question\n",
    "    })\n",
    "\n",
    "    return response\n",
    "\n",
    "# qa_maths_reasoning(text,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG With Math Tool (Langgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "import numexpr\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "    Expression should be a single line mathematical expression\n",
    "    that solves the problem.\n",
    "    If proportion or portion is mentioned, give the answer as a percentage.\n",
    "\n",
    "    Examples:\n",
    "        \"37593 * 67\" for \"37593 times 67\"\n",
    "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "    \"\"\"\n",
    "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "    return str(\n",
    "        numexpr.evaluate(\n",
    "            expression.strip(),\n",
    "            global_dict={},  # restrict access to globals\n",
    "            local_dict=local_dict,  # add common mathematical functions\n",
    "        )\n",
    "    )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "tools = [calculator]\n",
    "# Remove the tool_choice parameter for older API versions\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "class ChainState(TypedDict):\n",
    "    \"\"\"LangGraph state.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    ground_truth: str\n",
    "    score_reasoning: str\n",
    "    score: bool\n",
    "    response: str\n",
    "\n",
    "def call_chain(state: ChainState, config: RunnableConfig):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def call_model(state: ChainState, config: RunnableConfig):\n",
    "    response = llm.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def clean_response(state: ChainState, config: RunnableConfig) -> str:\n",
    "    question = state['messages'][0].content\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    prompt_str = \"\"\"\n",
    "    Give only the final answer without re-stating the question.\n",
    "    \\nResponse:\\n{messages}\\n\n",
    "    Question:\\n{question}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"messages\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'question':question,'messages':last_message})\n",
    "\n",
    "    return {\"response\":response, \"messages\": [response],\"ground_truth\":ground_truth}\n",
    "\n",
    "def llm_score(state: ChainState, config: RunnableConfig) -> str:\n",
    "    \"\"\"Use the LLM to score the text and question.\n",
    "    \"\"\"\n",
    "\n",
    "    # prompt_str = \"\"\"\n",
    "    # Return True if the ground truth is in the model answer or False if it is not. \n",
    "    # Compare the model answer and the ground truth with tolerance.\n",
    "    # Ground truth: {ground_truth}\\n\n",
    "    # Model answer: {answer}\\n\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt_str = \"\"\"\n",
    "    Compare the numerical values in the ground truth and model answer to decide if they are the same answer.\n",
    "    Follow these steps:\n",
    "    1. Extract the numerical value from both answers. Ignore the units.\n",
    "    2. Return True if the absolute values are equivalent within a reasonable margin, False otherwise\n",
    "    \n",
    "    Ground truth: {ground_truth}\n",
    "    Model answer: {answer}\n",
    "    \n",
    "    Let's solve this step by step:\n",
    "    1. Ground truth number: [extract number]\n",
    "    2. Compare with tolerance: [comparison result]\n",
    "    \n",
    "    Final answer (True/False): \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"answer\", \"ground_truth\"],\n",
    "    )\n",
    "\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    # print(state['messages'][1].content)\n",
    "    query = state['messages'][1].content\n",
    "    answer = state['response']\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    score = chain.invoke({\n",
    "        # 'query':query,\n",
    "        'answer':answer,\n",
    "        'ground_truth':ground_truth\n",
    "        })\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"response\":state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\":ground_truth, \n",
    "        'score_reasoning':score\n",
    "        }\n",
    "\n",
    "def clean_response_score(state: ChainState, config: RunnableConfig) -> str:\n",
    "    prompt_str = \"\"\"\n",
    "    Give only the final answer (True/False) from the score response.\n",
    "    \\nResponse:\\n{score_reasoning}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"score\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'score_reasoning':state['score_reasoning'],})\n",
    "\n",
    "    return {\n",
    "        \"response\":state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\":state['ground_truth'], \n",
    "        'score_reasoning':state['score_reasoning'],\n",
    "        'score':response\n",
    "        }\n",
    "\n",
    "graph_builder = StateGraph(ChainState)\n",
    "graph_builder.add_node(\"call_tool\", call_chain)\n",
    "graph_builder.add_node(\"execute_tool\", ToolNode(tools))\n",
    "graph_builder.add_node(\"call_model\", call_model)\n",
    "graph_builder.add_node(\"clean_response\", clean_response)\n",
    "graph_builder.add_node(\"llm_score\", llm_score)\n",
    "graph_builder.add_node(\"clean_response_score\", clean_response_score)\n",
    "graph_builder.set_entry_point(\"call_tool\")\n",
    "graph_builder.add_edge(\"call_tool\", \"execute_tool\")\n",
    "graph_builder.add_edge(\"execute_tool\", \"call_model\")\n",
    "graph_builder.add_edge(\"call_model\", \"clean_response\")\n",
    "graph_builder.add_edge(\"clean_response\", 'llm_score')\n",
    "graph_builder.add_edge('llm_score','clean_response_score')\n",
    "graph_builder.add_edge('clean_response_score',END)\n",
    "chain = graph_builder.compile()\n",
    "\n",
    "def qa_maths_reasoning_langgraph(text: str, question: str, ground_truth):\n",
    "    query = f\"\"\"read the following text:\\n---{text}\\n---\\nQuestion: {question}\"\"\"\n",
    "    result = chain.invoke({'messages': ['user', query], 'ground_truth': ground_truth})\n",
    "    result['question'] = question\n",
    "    return result\n",
    "\n",
    "# display(Image(chain.get_graph().draw_mermaid_png()))\n",
    "# result = qa_maths_reasoning_langgraph(text, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils.qa_advanced_scorer' has no attribute 'qa_maths_reasoning_langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 243\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# display(Image(chain.get_graph().draw_mermaid_png()))\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_advanced_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqa_maths_reasoning_langgraph\u001b[49m(text, question, answer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils.qa_advanced_scorer' has no attribute 'qa_maths_reasoning_langgraph'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numexpr\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "    Expression should be a single line mathematical expression\n",
    "    that solves the problem.\n",
    "    If proportion or portion is mentioned, give the answer as a percentage.\n",
    "\n",
    "    Examples:\n",
    "        \"37593 * 67\" for \"37593 times 67\"\n",
    "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "    \"\"\"\n",
    "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "    return str(\n",
    "        numexpr.evaluate(\n",
    "            expression.strip(),\n",
    "            global_dict={},  # restrict access to globals\n",
    "            local_dict=local_dict,  # add common mathematical functions\n",
    "        )\n",
    "    )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "tools = [calculator]\n",
    "# Remove the tool_choice parameter for older API versions\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "class ChainState(TypedDict):\n",
    "    \"\"\"LangGraph state with enhanced scoring.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    ground_truth: str\n",
    "    score_reasoning: str\n",
    "    detailed_score: dict\n",
    "    consensus_score: bool\n",
    "    overall_score: float\n",
    "    response: str\n",
    "\n",
    "def call_chain(state: ChainState, config: RunnableConfig):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def call_model(state: ChainState, config: RunnableConfig):\n",
    "    response = llm.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def clean_response(state: ChainState, config: RunnableConfig) -> str:\n",
    "    question = state['messages'][0].content\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    prompt_str = \"\"\"\n",
    "    Give only the final answer with its unit where relevant without re-stating the question.\n",
    "    \\nResponse:\\n{messages}\\n\n",
    "    Question:\\n{question}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"messages\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'question':question,'messages':last_message})\n",
    "\n",
    "    return {\"response\":response, \"messages\": [response],\"ground_truth\":ground_truth}\n",
    "\n",
    "class ChainState(TypedDict):\n",
    "    \"\"\"LangGraph state with enhanced scoring.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    ground_truth: str\n",
    "    score_reasoning: str\n",
    "    detailed_score: dict\n",
    "    consensus_score: bool\n",
    "    response: str\n",
    "\n",
    "def advanced_scorer(state: ChainState, config: RunnableConfig) -> str:\n",
    "    \"\"\"Advanced scoring agent that considers multiple aspects of the answer.\"\"\"\n",
    "    \n",
    "    prompt_str = \"\"\"\n",
    "    Perform a detailed analysis of the model's answer compared to the ground truth.\n",
    "    Consider multiple aspects in your evaluation:\n",
    "    \n",
    "    1. Numerical Accuracy:\n",
    "       - Extract and compare numerical values\n",
    "       - Consider acceptable margin of error (±1% or correct to the whole number)\n",
    "       - Check for unit consistency\n",
    "    \n",
    "    2. Conceptual Correctness:\n",
    "       - Verify if the approach/methodology is correct\n",
    "       - Check if all required components are present\n",
    "    \n",
    "    3. Context Relevance:\n",
    "       - Ensure the answer addresses the specific question\n",
    "       - Verify if any contextual requirements are met\n",
    "    \n",
    "    Question: {query}\n",
    "    Ground truth: {ground_truth}\n",
    "    Model answer: {answer}\n",
    "    \n",
    "    Analyze the response and return a JSON object with this exact structure:\n",
    "    {{\n",
    "        \"numerical_accuracy\": {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<explanation>\"\n",
    "        }},\n",
    "        \"conceptual_correctness\": {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<explanation>\"\n",
    "        }},\n",
    "        \"context_relevance\": {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<explanation>\"\n",
    "        }},\n",
    "        \"overall_score\": <float between 0 and 1>,\n",
    "        \"is_correct\": <boolean>\n",
    "    }}\n",
    "    \n",
    "    Ensure your response is a valid JSON object matching this structure exactly.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"query\", \"answer\", \"ground_truth\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | JsonOutputParser()\n",
    "\n",
    "    detailed_score = chain.invoke({\n",
    "        'query': state['messages'][0].content,\n",
    "        'answer': state['response'],\n",
    "        'ground_truth': state['ground_truth']\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"response\": state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\": state['ground_truth'],\n",
    "        \"detailed_score\": detailed_score\n",
    "    }\n",
    "\n",
    "def consensus_scorer(state: ChainState, config: RunnableConfig) -> str:\n",
    "    \"\"\"Combine original score with advanced score for final decision.\"\"\"\n",
    "    \n",
    "    prompt_str = \"\"\"\n",
    "    Analyze both scoring approaches and provide a final consensus.\n",
    "    \n",
    "    Original scoring reasoning:\n",
    "    {original_score}\n",
    "    \n",
    "    Detailed scoring analysis:\n",
    "    {detailed_score}\n",
    "    \n",
    "    Based on both scoring methods, make a final decision.\n",
    "    Consider:\n",
    "    1. The original binary score\n",
    "    2. The detailed numerical accuracy score\n",
    "    3. The conceptual correctness score\n",
    "    4. The context relevance score\n",
    "    \n",
    "    Provide your response in exactly this format:\n",
    "    DECISION: [True/False]\n",
    "    REASONING: [Your brief explanation]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"original_score\", \"detailed_score\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    consensus = chain.invoke({\n",
    "        'original_score': state['score_reasoning'],\n",
    "        'detailed_score': str(state['detailed_score'])\n",
    "    })\n",
    "\n",
    "    final_decision = consensus.split('\\n')[0].replace('DECISION:', '').strip() == 'True'\n",
    "\n",
    "    return {\n",
    "        \"response\": state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\": state['ground_truth'],\n",
    "        \"score_reasoning\": state['score_reasoning'],\n",
    "        \"detailed_score\": state['detailed_score'],\n",
    "        \"consensus_score\": final_decision\n",
    "    }\n",
    "\n",
    "# Update the graph with new scoring nodes\n",
    "graph_builder = StateGraph(ChainState)\n",
    "graph_builder.add_node(\"call_tool\", call_chain)\n",
    "graph_builder.add_node(\"execute_tool\", ToolNode(tools))\n",
    "graph_builder.add_node(\"call_model\", call_model)\n",
    "graph_builder.add_node(\"clean_response\", clean_response)\n",
    "graph_builder.add_node(\"llm_score\", llm_score)\n",
    "graph_builder.add_node(\"advanced_scorer\", advanced_scorer)\n",
    "graph_builder.add_node(\"consensus_scorer\", consensus_scorer)\n",
    "graph_builder.set_entry_point(\"call_tool\")\n",
    "\n",
    "# Define the enhanced flow\n",
    "graph_builder.add_edge(\"call_tool\", \"execute_tool\")\n",
    "graph_builder.add_edge(\"execute_tool\", \"call_model\")\n",
    "graph_builder.add_edge(\"call_model\", \"clean_response\")\n",
    "graph_builder.add_edge(\"clean_response\", \"llm_score\")\n",
    "graph_builder.add_edge(\"llm_score\", \"advanced_scorer\")\n",
    "graph_builder.add_edge(\"advanced_scorer\", \"consensus_scorer\")\n",
    "graph_builder.add_edge(\"consensus_scorer\", END)\n",
    "\n",
    "chain = graph_builder.compile()\n",
    "\n",
    "def qa_maths_reasoning_langgraph_advanced_scorer(text: str, question: str, ground_truth):\n",
    "    query = f\"\"\"read the following text:\\n---{text}\\n---\\nQuestion: {question}\"\"\"\n",
    "    result = chain.invoke({'messages': ['user', query], 'ground_truth': ground_truth})\n",
    "    result['question'] = question\n",
    "    result['overall_score'] = result['detailed_score']['overall_score']\n",
    "    return result\n",
    "\n",
    "# display(Image(chain.get_graph().draw_mermaid_png()))\n",
    "# result = qa_maths_reasoning_langgraph(text, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the difference in percentage cumulative return on investment for United Parcel Service Inc. compared to the S&P 500 index for the five-year period ended 12/31/09. The cumulative returns are -24.05% for UPS and 2.11% for the S&P 500. The difference can be calculated as follows: \n",
      "\n",
      "Difference = UPS Return - S&P 500 Return\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"calculator\",\n",
      "  \"action_input\": \"(-24.05 - 2.11)\"\n",
      "}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m-26.16\u001b[0m\u001b[32;1m\u001b[1;3mThought: I have calculated the difference in cumulative return on investment for United Parcel Service Inc. compared to the S&P 500 index, which is -26.16%. This indicates that UPS underperformed the S&P 500 by this percentage over the five-year period ended 12/31/09. \n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The difference in percentage cumulative return on investment for United Parcel Service Inc. compared to the S&P 500 index for the five-year period ended 12/31/09 is -26.16%.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "reload(qa_advanced_scorer)\n",
    "\n",
    "# text = \"\"\"\n",
    "# gain or loss on ownership change in map results from contributions to map of certain environmental capital expenditures and leased property acquisitions funded by marathon and ashland .\n",
    "# in accordance with map 2019s limited liability company agreement , in certain instances , environmental capital expenditures and acquisitions of leased properties are funded by the original contributor of the assets , but no change in ownership interest may result from these contributions .\n",
    "# an excess of ashland funded improvements over marathon funded improvements results in a net gain and an excess of marathon funded improvements over ashland funded improvements results in a net loss .\n",
    "# cost of revenues increased by $ 5.822 billion in 2004 from 2003 and by $ 6.040 billion in 2003 from 2002 .\n",
    "# the increases are primarily in the rm&t segment and result from higher acquisition costs for crude oil , refined products , refinery charge and blend feedstocks and increased manufacturing expenses .\n",
    "# selling , general and administrative expenses increased by $ 105 million in 2004 from 2003 and by $ 97 million in 2003 from 2002 .\n",
    "# the increase in 2004 was primarily due to increased stock-based compensation and higher costs associated with business transformation and outsourcing .\n",
    "# our 2004 results were also impacted by start-up costs associated with the lng project in equatorial guinea and the increased cost of complying with governmental regulations .\n",
    "# the increase in 2003 was primarily due to increased employee benefit expenses ( caused by increased pension expense resulting from changes in actuarial assumptions and a decrease in realized returns on plan assets ) and other employee related costs .\n",
    "# additionally , during 2003 , we recorded a charge of $ 24 million related to organizational and business process changes .\n",
    "# inventory market valuation reserve ( 2018 2018imv 2019 2019 ) is established to reduce the cost basis of inventories to current market value .\n",
    "# generally , we will establish an imv reserve when crude oil prices fall below $ 22 per barrel .\n",
    "# the 2002 results of operations include credits to income from operations of $ 71 million , reversing the imv reserve at december 31 , 2001 .\n",
    "# net interest and other financial costs decreased by $ 25 million in 2004 from 2003 and by $ 82 million in 2003 from 2002 .\n",
    "# the decrease in 2004 is primarily due to an increase in interest income .\n",
    "# the decrease in 2003 is primarily due to an increase in capitalized interest related to increased long-term construction projects , the favorable effect of interest rate swaps , the favorable effect of a reduction in interest on tax deficiencies and increased interest income on investments .\n",
    "# additionally , included in net interest and other financing costs are foreign currency gains of $ 9 million , $ 13 million and $ 8 million for 2004 , 2003 and 2002 .\n",
    "# loss from early extinguishment of debt in 2002 was attributable to the retirement of $ 337 million aggregate principal amount of debt , resulting in a loss of $ 53 million .\n",
    "# minority interest in income of map , which represents ashland 2019s 38 percent ownership interest , increased by $ 230 million in 2004 from 2003 and by $ 129 million in 2003 from 2002 .\n",
    "# map income was higher in 2004 compared to 2003 and in 2003 compared to 2002 as discussed below in the rm&t segment .\n",
    "# minority interest in loss of equatorial guinea lng holdings limited , which represents gepetrol 2019s 25 percent ownership interest , was $ 7 million in 2004 , primarily resulting from gepetrol 2019s share of start-up costs associated with the lng project in equatorial guinea .\n",
    "# provision for income taxes increased by $ 143 million in 2004 from 2003 and by $ 215 million in 2003 from 2002 , primarily due to $ 388 million and $ 720 million increases in income before income taxes .\n",
    "# the effective tax rate for 2004 was 36.6 percent compared to 36.6 percent and 42.1 percent for 2003 and 2002 .\n",
    "# the higher rate in 2002 was due to the united kingdom enactment of a supplementary 10 percent tax on profits from the north sea oil and gas production , retroactively effective to april 17 , 2002 .\n",
    "# in 2002 , we recognized a one-time noncash deferred tax adjustment of $ 61 million as a result of the rate increase .\n",
    "# the following is an analysis of the effective tax rate for the periods presented: .\n",
    "\n",
    "# table:\n",
    "\n",
    "# [['', '2004', '2003', '2002'], ['statutory tax rate', '35.0% ( 35.0 % )', '35.0% ( 35.0 % )', '35.0% ( 35.0 % )'], ['effects of foreign operations ( a )', '1.3', '-0.4 ( 0.4 )', '5.6'], ['state and local income taxes after federal income tax effects', '1.6', '2.2', '3.9'], ['other federal tax effects', '-1.3 ( 1.3 )', '-0.2 ( 0.2 )', '-2.4 ( 2.4 )'], ['effective tax rate', '36.6% ( 36.6 % )', '36.6% ( 36.6 % )', '42.1% ( 42.1 % )']]\n",
    "\n",
    "# ( a ) the deferred tax effect related to the enactment of a supplemental tax in the u.k .\n",
    "# increased the effective tax rate 7.0 percent in .\n",
    "# \"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "( 1 ) includes shares repurchased through our publicly announced share repurchase program and shares tendered to pay the exercise price and tax withholding on employee stock options .\n",
    "shareowner return performance graph the following performance graph and related information shall not be deemed 201csoliciting material 201d or to be 201cfiled 201d with the securities and exchange commission , nor shall such information be incorporated by reference into any future filing under the securities act of 1933 or securities exchange act of 1934 , each as amended , except to the extent that the company specifically incorporates such information by reference into such filing .\n",
    "the following graph shows a five-year comparison of cumulative total shareowners 2019 returns for our class b common stock , the s&p 500 index , and the dow jones transportation average .\n",
    "the comparison of the total cumulative return on investment , which is the change in the quarterly stock price plus reinvested dividends for each of the quarterly periods , assumes that $ 100 was invested on december 31 , 2004 in the s&p 500 index , the dow jones transportation average , and our class b common stock .\n",
    "comparison of five year cumulative total return $ 40.00 $ 60.00 $ 80.00 $ 100.00 $ 120.00 $ 140.00 $ 160.00 2004 20092008200720062005 s&p 500 ups dj transport .\n",
    "\n",
    "table:\n",
    "\n",
    "[['', '12/31/04', '12/31/05', '12/31/06', '12/31/07', '12/31/08', '12/31/09'], ['united parcel service inc .', '$ 100.00', '$ 89.49', '$ 91.06', '$ 87.88', '$ 70.48', '$ 75.95'], ['s&p 500 index', '$ 100.00', '$ 104.91', '$ 121.48', '$ 128.15', '$ 80.74', '$ 102.11'], ['dow jones transportation average', '$ 100.00', '$ 111.65', '$ 122.61', '$ 124.35', '$ 97.72', '$ 115.88']]\n",
    "\n",
    ".\n",
    "\"\"\"\n",
    "\n",
    "# question = \"by what percent did effects of foreign operations decrease from 2002 to 2004?\"\n",
    "question = \"what was the difference in percentage cumulative return on investment for united parcel service inc . compared to the s&p 500 index for the five year period ended 12/31/09?\"\n",
    "\n",
    "# ground_truth = \"-76.8%\"\n",
    "ground_truth = \"-26.16%\"\n",
    "\n",
    "# result = qa_maths_reasoning_langgraph(text, question, ground_truth)\n",
    "result = qa_advanced_scorer.qa_maths_reasoning_langgraph_advanced_scorer(text, question, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='user', additional_kwargs={}, response_metadata={}, id='e0fddfb9-643e-410b-80e1-226d77b6829a')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def score_data(data, verbose=False, convert_to_df=True, advanced=True):\n",
    "    result_list = []\n",
    "    for i,r in data.iterrows():\n",
    "\n",
    "        text =r['source_text']\n",
    "        qa = r['qa_exploded']\n",
    "\n",
    "        if str(qa)!='nan':\n",
    "            result = {}\n",
    "            question = qa['question']\n",
    "            answer = qa['answer']\n",
    "            if advanced:\n",
    "                response = qa_advanced_scorer.qa_maths_reasoning_langgraph_advanced_scorer(text,question, answer)\n",
    "                answer_llm = response['response']\n",
    "                score = response['detailed_score']['overall_score']\n",
    "            else:\n",
    "                response = qa_basic_scorer.qa_maths_reasoning_langgraph(text,question, answer)#[-1].content\n",
    "                answer_llm = response['response']\n",
    "                display(response)\n",
    "                score = response['score']\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Question: {question}')\n",
    "                print(f'Answer (Ground Truth): {answer}')\n",
    "                print(f'Answer (LLM): {answer_llm}')\n",
    "                print(f'Score: {score}')\n",
    "                print('\\n')\n",
    "\n",
    "            result_list.append(response)\n",
    "\n",
    "        else:\n",
    "            print('no qa')\n",
    "\n",
    "    if convert_to_df:\n",
    "        return pd.DataFrame(result_list)\n",
    "\n",
    "    return result_list\n",
    "\n",
    "def process_row(row, verbose=False, advanced=False):\n",
    "    text = row['source_text']\n",
    "    qa = row['qa_exploded']\n",
    "\n",
    "    if str(qa) != 'nan':\n",
    "        question = qa['question']\n",
    "        answer = qa['answer']\n",
    "        print(question)\n",
    "        if advanced:\n",
    "            response = qa_advanced_scorer.qa_maths_reasoning_langgraph_advanced_scorer(text, question, answer)\n",
    "            answer_llm = response['response']\n",
    "            score = response['detailed_score']['overall_score']\n",
    "        else:\n",
    "            response = qa_basic_scorer.qa_maths_reasoning_langgraph(text, question, answer)\n",
    "            answer_llm = response['response']\n",
    "            score = response['score']\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Question: {question}')\n",
    "            print(f'Answer (Ground Truth): {answer}')\n",
    "            print(f'Answer (LLM): {answer_llm}')\n",
    "            print(f'Score: {score}')\n",
    "            print('\\n')\n",
    "\n",
    "        return response\n",
    "    else:\n",
    "        print('no qa')\n",
    "        return None\n",
    "\n",
    "def score_data_concurrently(data, verbose=False, convert_to_df=True, max_workers=5, advanced=False):\n",
    "    result_list = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_row = {executor.submit(process_row, row, verbose,advanced=advanced): i for i, row in data.iterrows()}\n",
    "\n",
    "        for future in as_completed(future_to_row):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                result_list.append(result)\n",
    "\n",
    "    if convert_to_df:\n",
    "        return pd.DataFrame(result_list)\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# df_scores = score_data(data_clean[:1], advanced=True)\n",
    "# df_scores = score_data_concurrently(data_clean[:5],advanced_scorer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in december 2012 what was the percentage difference in the carrying values of the long-term debt excluding current portion\n",
      "what percentage of total non-recourse debt as of december 31 , 2010 is due in 2012?\n",
      "what percent of the muilti asset value is from the asset allocation and balanced section?\n",
      "what was the total collateral of all types december 31 , 2009?\n",
      "what was the rate of the income tax benefit based on the stock compensation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores: {'accuracy': 0.72}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "reload(qa_advanced_scorer)\n",
    "\n",
    "def process_sample(sample_df):\n",
    "    # Calculate F1 score for the current sample based on 'score' column\n",
    "    y_true = sample_df['overall_score']\n",
    "    y_pred = [1] * len(y_true)  # assuming `True` indicates correct predictions by the model\n",
    "\n",
    "    # Calculate multiple metrics\n",
    "    metrics = {\n",
    "        'accuracy': np.mean(y_true),\n",
    "    }\n",
    "    return metrics, sample_df\n",
    "\n",
    "def evaluate_samples(data, k, n, max_workers=5, advanced=False, concurrent=False):\n",
    "    metrics_list = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    def get_random_sample():\n",
    "        # Randomly sample `n` rows\n",
    "        # df_scores = score_data_concurrently(data.sample(n=n),advanced=advanced)\n",
    "        if concurrent:\n",
    "            df_scores = score_data_concurrently(data.sample(n=n), advanced=advanced)\n",
    "        else:\n",
    "            df_scores = score_data(data.sample(n=n), advanced=advanced)\n",
    "        # display(df_scores)\n",
    "        # df_scores['score'] = df_scores['score'].apply(lambda x: 1 if str(x)=='True' else 0)\n",
    "        # display(df_scores)\n",
    "        return df_scores\n",
    "\n",
    "    # Use ThreadPoolExecutor to calculate F1 scores concurrently\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create a list of future tasks for each sample\n",
    "        futures = [executor.submit(process_sample, get_random_sample()) for _ in range(k)]\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            metrics, sample_df = future.result()\n",
    "            metrics_list.append(metrics)\n",
    "            sample_dfs.append(sample_df)\n",
    "\n",
    "    # Aggregate results by averaging each metric across all samples\n",
    "    aggregated_metrics = {\n",
    "        'accuracy': np.mean([metrics['accuracy'] for metrics in metrics_list]),\n",
    "    }\n",
    "\n",
    "    concatenated_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "\n",
    "    return aggregated_metrics, concatenated_df\n",
    "\n",
    "# Example usage:\n",
    "k = 1 # Number of random samples to test\n",
    "n = 5   # Number of rows in each sample\n",
    "metrics, df_scores = evaluate_samples(data_clean, k, n, advanced=True, concurrent=True)\n",
    "print(\"F1 Scores:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.000000\n",
       "mean     0.720000\n",
       "std      0.438178\n",
       "min      0.000000\n",
       "25%      0.600000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000\n",
       "Name: overall_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.72}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_scores['overall_score'].describe(), metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: 127\n",
      "Question: what was the change in unrecognized tax benefits between 2008 and 2009?\n",
      "Response: -$127 million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': 'The model answer of -$127 million does not match the ground truth of 127. The absolute values do not match closely, and the negative sign indicates a decline, which is not equivalent to the positive ground truth value.'},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not correctly represent the numerical value requested. The methodology of providing a negative value when a positive value was expected is incorrect.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': 'The model answer does not address the specific question as it provides a negative value instead of the expected positive value. The context of the question is not met.'},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 0%\n",
      "Question: now much of the net increase in aro during the period was due to accretion , in millions?\n",
      "Response: 0 million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': 'The ground truth is 0%, while the model answer is 0 million. These values are not numerically equivalent, as 0% represents a proportion and 0 million represents a quantity. They do not match within the acceptable margin of error.'},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not correctly address the question as it provides a quantity (0 million) instead of a percentage (0%). The methodology of interpreting the question is incorrect.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question regarding a percentage. The context of the question requires a percentage response, which the model fails to provide.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for i,r in df_scores.loc[df_scores['overall_score']==0].iterrows():\n",
    "    ground_truth = r['ground_truth']\n",
    "    question = r['question']\n",
    "    # score_reasoning = r['score_reasoning']\n",
    "    detailed_score = r['detailed_score']\n",
    "    score = r['overall_score']\n",
    "    response = r['response']\n",
    "    print(f'Ground Truth: {ground_truth}')\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Response: {response}')\n",
    "    # print(f'Score reasoning: {score_reasoning}')\n",
    "    display(detailed_score)\n",
    "    print(f'Score: {score}')\n",
    "    print('\\n-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
