{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "from utils import qa_advanced_scorer, qa_basic_scorer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "data = pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data.loc[data['qa'].isna()]['qa_0'].iloc[0])\n",
    "# display(data.loc[data['qa'].isna()]['qa_1'].iloc[0])\n",
    "# display(data.loc[data['qa'].isna()]['annotation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data.head(1)['qa'][0]['question'])\n",
    "# display(data.head(1)['annotation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_source_text(x):\n",
    "    pre_text = '\\n'.join(x['pre_text'])\n",
    "    table = x['table']\n",
    "    post_text = '\\n'.join(x['post_text'])\n",
    "    return f\"{pre_text}\\n\\ntable:\\n\\n{table}\\n\\n{post_text}\"\n",
    "\n",
    "def split_qa(x):\n",
    "    qa = x['qa']\n",
    "    qa_0 = x['qa_0']\n",
    "    qa_1 = x['qa_1']\n",
    "\n",
    "    qa_list = []\n",
    "\n",
    "    if str(qa) != 'nan':\n",
    "        qa_list.append(qa)\n",
    "    else:\n",
    "        if str(qa_0) != 'nan':\n",
    "            qa_list.append(qa_0)\n",
    "        if str(qa_1) != 'nan':\n",
    "            qa_list.append(qa_1)\n",
    "    \n",
    "    return qa_list\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_json('train.json')\n",
    "    data['source_text'] = data.apply(get_source_text, axis=1)\n",
    "    data['qa_exploded'] = data.apply(split_qa, axis=1)\n",
    "    data = data.explode('qa_exploded', ignore_index=True)\n",
    "    data['question'] = data['qa_exploded'].apply(lambda x: x['question'] if str(x)!='nan' else None)\n",
    "    data['answer'] = data['qa_exploded'].apply(lambda x: x['answer'] if str(x)!='nan' else None)\n",
    "    return data\n",
    "\n",
    "data_clean = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3965, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_text</th>\n",
       "      <th>post_text</th>\n",
       "      <th>filename</th>\n",
       "      <th>table_ori</th>\n",
       "      <th>table</th>\n",
       "      <th>qa</th>\n",
       "      <th>id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>qa_0</th>\n",
       "      <th>qa_1</th>\n",
       "      <th>source_text</th>\n",
       "      <th>qa_exploded</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[26 | 2009 annual report in fiscal 2008 , reve...</td>\n",
       "      <td>[year ended june 30 , cash provided by operati...</td>\n",
       "      <td>JKHY/2009/page_28.pdf</td>\n",
       "      <td>[[, Year ended June 30, 2009], [2008, 2007], [...</td>\n",
       "      <td>[[2008, year ended june 30 2009 2008, year end...</td>\n",
       "      <td>{'question': 'what was the percentage change i...</td>\n",
       "      <td>Single_JKHY/2009/page_28.pdf-3</td>\n",
       "      <td>{'amt_table': '&lt;table class='wikitable'&gt;&lt;tr&gt;&lt;t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 | 2009 annual report in fiscal 2008 , reven...</td>\n",
       "      <td>{'question': 'what was the percentage change i...</td>\n",
       "      <td>what was the percentage change in the net cash...</td>\n",
       "      <td>14.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[substantially all of the goodwill and other i...</td>\n",
       "      <td>[the above unaudited pro forma financial infor...</td>\n",
       "      <td>RSG/2008/page_114.pdf</td>\n",
       "      <td>[[, Year Ended December 31, 2008 (Unaudited), ...</td>\n",
       "      <td>[[, year ended december 31 2008 ( unaudited ),...</td>\n",
       "      <td>{'question': 'what was the percent of the grow...</td>\n",
       "      <td>Single_RSG/2008/page_114.pdf-2</td>\n",
       "      <td>{'amt_table': '&lt;table class='wikitable'&gt;&lt;tr&gt;&lt;t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>substantially all of the goodwill and other in...</td>\n",
       "      <td>{'question': 'what was the percent of the grow...</td>\n",
       "      <td>what was the percent of the growth in the reve...</td>\n",
       "      <td>1.3%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pre_text  \\\n",
       "0  [26 | 2009 annual report in fiscal 2008 , reve...   \n",
       "1  [substantially all of the goodwill and other i...   \n",
       "\n",
       "                                           post_text               filename  \\\n",
       "0  [year ended june 30 , cash provided by operati...  JKHY/2009/page_28.pdf   \n",
       "1  [the above unaudited pro forma financial infor...  RSG/2008/page_114.pdf   \n",
       "\n",
       "                                           table_ori  \\\n",
       "0  [[, Year ended June 30, 2009], [2008, 2007], [...   \n",
       "1  [[, Year Ended December 31, 2008 (Unaudited), ...   \n",
       "\n",
       "                                               table  \\\n",
       "0  [[2008, year ended june 30 2009 2008, year end...   \n",
       "1  [[, year ended december 31 2008 ( unaudited ),...   \n",
       "\n",
       "                                                  qa  \\\n",
       "0  {'question': 'what was the percentage change i...   \n",
       "1  {'question': 'what was the percent of the grow...   \n",
       "\n",
       "                               id  \\\n",
       "0  Single_JKHY/2009/page_28.pdf-3   \n",
       "1  Single_RSG/2008/page_114.pdf-2   \n",
       "\n",
       "                                          annotation qa_0 qa_1  \\\n",
       "0  {'amt_table': '<table class='wikitable'><tr><t...  NaN  NaN   \n",
       "1  {'amt_table': '<table class='wikitable'><tr><t...  NaN  NaN   \n",
       "\n",
       "                                         source_text  \\\n",
       "0  26 | 2009 annual report in fiscal 2008 , reven...   \n",
       "1  substantially all of the goodwill and other in...   \n",
       "\n",
       "                                         qa_exploded  \\\n",
       "0  {'question': 'what was the percentage change i...   \n",
       "1  {'question': 'what was the percent of the grow...   \n",
       "\n",
       "                                            question answer  \n",
       "0  what was the percentage change in the net cash...  14.1%  \n",
       "1  what was the percent of the growth in the reve...   1.3%  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_clean.shape)\n",
    "data_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$12.0 million'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qa_simple_rag(text,question):\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "        api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "        temperature=0.15,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        # max_retries=3,\n",
    "    )\n",
    "\n",
    "    prompt_str = \"\"\"\n",
    "    You are reading a financial document. \n",
    "    Provide an answer to the following question based on the information provided in the text.\n",
    "    Give the answer only.\n",
    "    \\n\\n{text}\\n\\n\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"text\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\n",
    "        'text': text,\n",
    "        'question':question\n",
    "    })\n",
    "\n",
    "    return response\n",
    "\n",
    "qa_simple_rag(text,question)\n",
    "# qa_simple_rag('in 2022, the company made 200 pounds after spending 100','what is the total profit made in 2022?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG With Math Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (LLMathChain is deprecated...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langchain_experimental\\utilities\\__init__.py:2: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_experimental.utilities.python import PythonREPL\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "import numexpr\n",
    "\n",
    "def qa_maths_reasoning(text,question):\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "        api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "        temperature=0.15,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        # max_retries=3,\n",
    "    )\n",
    "    problem_chain = LLMMathChain.from_llm(llm=llm)\n",
    "    math_tool = Tool.from_function(name=\"Calculator\",\n",
    "                    func=problem_chain.run,\n",
    "                    description=\"\"\"Useful for when you need to answer questions \n",
    "                    about math. This tool is only for math questions and nothing else. Only input\n",
    "                    math expressions.\"\"\")\n",
    "\n",
    "    @tool\n",
    "    def calculator(expression: str) -> str:\n",
    "        \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "        Expression should be a single line mathematical expression\n",
    "        that solves the problem.\n",
    "\n",
    "        Examples:\n",
    "            \"37593 * 67\" for \"37593 times 67\"\n",
    "            \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "        \"\"\"\n",
    "        local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "        return str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={},  # restrict access to globals\n",
    "                local_dict=local_dict,  # add common mathematical functions\n",
    "            )\n",
    "        )\n",
    "\n",
    "    word_problem_template = \"\"\"You are a reasoning agent tasked with solving \n",
    "    the user's logic-based questions. Logically arrive at the solution, and be \n",
    "    factual. In your answers, clearly detail the steps involved and give the \n",
    "    final answer. Provide the response in bullet points. \n",
    "    Question  {question}\"\"\"\n",
    "\n",
    "    math_assistant_prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                                        template=word_problem_template\n",
    "                                        )\n",
    "    word_problem_chain = LLMChain(llm=llm,\n",
    "                                prompt=math_assistant_prompt)\n",
    "    word_problem_tool = Tool.from_function(name=\"Reasoning Tool\",\n",
    "                                        func=word_problem_chain.run,\n",
    "                                        description=\"Useful for when you need to answer logic-based/reasoning questions.\",\n",
    "                                        )\n",
    "\n",
    "    python_repl = PythonREPL()\n",
    "    repl_tool = Tool(\n",
    "        name=\"python_repl\",\n",
    "        description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "        func=python_repl.run,\n",
    "    )\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=[math_tool,calculator,word_problem_tool,repl_tool],\n",
    "        # tools=[math_tool],\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=False,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    prompt_str = \"\"\"\n",
    "    You are reading a financial document. \n",
    "    Provide an answer to the following question based on the information provided in the text.\n",
    "    Give the answer only without repeating the question or statement.\n",
    "    \\n\\n{text}\\n\\n\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"text\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | agent \n",
    "\n",
    "    response = chain.invoke({\n",
    "        'text': text,\n",
    "        'question':question\n",
    "    })\n",
    "\n",
    "    return response\n",
    "\n",
    "# qa_maths_reasoning(text,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG With Math Tool (Langgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "import numexpr\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "    Expression should be a single line mathematical expression\n",
    "    that solves the problem.\n",
    "    If proportion or portion is mentioned, give the answer as a percentage.\n",
    "\n",
    "    Examples:\n",
    "        \"37593 * 67\" for \"37593 times 67\"\n",
    "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "    \"\"\"\n",
    "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "    return str(\n",
    "        numexpr.evaluate(\n",
    "            expression.strip(),\n",
    "            global_dict={},  # restrict access to globals\n",
    "            local_dict=local_dict,  # add common mathematical functions\n",
    "        )\n",
    "    )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "tools = [calculator]\n",
    "# Remove the tool_choice parameter for older API versions\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "class ChainState(TypedDict):\n",
    "    \"\"\"LangGraph state.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    ground_truth: str\n",
    "    score_reasoning: str\n",
    "    score: bool\n",
    "    response: str\n",
    "\n",
    "def call_chain(state: ChainState, config: RunnableConfig):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def call_model(state: ChainState, config: RunnableConfig):\n",
    "    response = llm.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def clean_response(state: ChainState, config: RunnableConfig) -> str:\n",
    "    question = state['messages'][0].content\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    prompt_str = \"\"\"\n",
    "    Give only the final answer without re-stating the question.\n",
    "    \\nResponse:\\n{messages}\\n\n",
    "    Question:\\n{question}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"messages\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'question':question,'messages':last_message})\n",
    "\n",
    "    return {\"response\":response, \"messages\": [response],\"ground_truth\":ground_truth}\n",
    "\n",
    "def llm_score(state: ChainState, config: RunnableConfig) -> str:\n",
    "    \"\"\"Use the LLM to score the text and question.\n",
    "    \"\"\"\n",
    "\n",
    "    # prompt_str = \"\"\"\n",
    "    # Return True if the ground truth is in the model answer or False if it is not. \n",
    "    # Compare the model answer and the ground truth with tolerance.\n",
    "    # Ground truth: {ground_truth}\\n\n",
    "    # Model answer: {answer}\\n\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt_str = \"\"\"\n",
    "    Compare the numerical values in the ground truth and model answer to decide if they are the same answer.\n",
    "    Follow these steps:\n",
    "    1. Extract the numerical value from both answers. Ignore the units.\n",
    "    2. Return True if the absolute values are equivalent within a reasonable margin, False otherwise\n",
    "    \n",
    "    Ground truth: {ground_truth}\n",
    "    Model answer: {answer}\n",
    "    \n",
    "    Let's solve this step by step:\n",
    "    1. Ground truth number: [extract number]\n",
    "    2. Compare with tolerance: [comparison result]\n",
    "    \n",
    "    Final answer (True/False): \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"answer\", \"ground_truth\"],\n",
    "    )\n",
    "\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    # print(state['messages'][1].content)\n",
    "    query = state['messages'][1].content\n",
    "    answer = state['response']\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    score = chain.invoke({\n",
    "        # 'query':query,\n",
    "        'answer':answer,\n",
    "        'ground_truth':ground_truth\n",
    "        })\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"response\":state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\":ground_truth, \n",
    "        'score_reasoning':score\n",
    "        }\n",
    "\n",
    "def clean_response_score(state: ChainState, config: RunnableConfig) -> str:\n",
    "    prompt_str = \"\"\"\n",
    "    Give only the final answer (True/False) from the score response.\n",
    "    \\nResponse:\\n{score_reasoning}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"score\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'score_reasoning':state['score_reasoning'],})\n",
    "\n",
    "    return {\n",
    "        \"response\":state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\":state['ground_truth'], \n",
    "        'score_reasoning':state['score_reasoning'],\n",
    "        'score':response\n",
    "        }\n",
    "\n",
    "graph_builder = StateGraph(ChainState)\n",
    "graph_builder.add_node(\"call_tool\", call_chain)\n",
    "graph_builder.add_node(\"execute_tool\", ToolNode(tools))\n",
    "graph_builder.add_node(\"call_model\", call_model)\n",
    "graph_builder.add_node(\"clean_response\", clean_response)\n",
    "graph_builder.add_node(\"llm_score\", llm_score)\n",
    "graph_builder.add_node(\"clean_response_score\", clean_response_score)\n",
    "graph_builder.set_entry_point(\"call_tool\")\n",
    "graph_builder.add_edge(\"call_tool\", \"execute_tool\")\n",
    "graph_builder.add_edge(\"execute_tool\", \"call_model\")\n",
    "graph_builder.add_edge(\"call_model\", \"clean_response\")\n",
    "graph_builder.add_edge(\"clean_response\", 'llm_score')\n",
    "graph_builder.add_edge('llm_score','clean_response_score')\n",
    "graph_builder.add_edge('clean_response_score',END)\n",
    "chain = graph_builder.compile()\n",
    "\n",
    "def qa_maths_reasoning_langgraph(text: str, question: str, ground_truth):\n",
    "    query = f\"\"\"read the following text:\\n---{text}\\n---\\nQuestion: {question}\"\"\"\n",
    "    result = chain.invoke({'messages': ['user', query], 'ground_truth': ground_truth})\n",
    "    result['question'] = question\n",
    "    return result\n",
    "\n",
    "# display(Image(chain.get_graph().draw_mermaid_png()))\n",
    "# result = qa_maths_reasoning_langgraph(text, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils.qa_advanced_scorer' has no attribute 'qa_maths_reasoning_langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 243\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# display(Image(chain.get_graph().draw_mermaid_png()))\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_advanced_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqa_maths_reasoning_langgraph\u001b[49m(text, question, answer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils.qa_advanced_scorer' has no attribute 'qa_maths_reasoning_langgraph'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numexpr\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate expression using Python's numexpr library.\n",
    "\n",
    "    Expression should be a single line mathematical expression\n",
    "    that solves the problem.\n",
    "    If proportion or portion is mentioned, give the answer as a percentage.\n",
    "\n",
    "    Examples:\n",
    "        \"37593 * 67\" for \"37593 times 67\"\n",
    "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
    "    \"\"\"\n",
    "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "    return str(\n",
    "        numexpr.evaluate(\n",
    "            expression.strip(),\n",
    "            global_dict={},  # restrict access to globals\n",
    "            local_dict=local_dict,  # add common mathematical functions\n",
    "        )\n",
    "    )\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "tools = [calculator]\n",
    "# Remove the tool_choice parameter for older API versions\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "class ChainState(TypedDict):\n",
    "    \"\"\"LangGraph state with enhanced scoring.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    ground_truth: str\n",
    "    score_reasoning: str\n",
    "    detailed_score: dict\n",
    "    consensus_score: bool\n",
    "    overall_score: float\n",
    "    response: str\n",
    "\n",
    "def call_chain(state: ChainState, config: RunnableConfig):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def call_model(state: ChainState, config: RunnableConfig):\n",
    "    response = llm.invoke(state[\"messages\"], config)\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    return {\"messages\": [response],\"ground_truth\":ground_truth,}\n",
    "\n",
    "def clean_response(state: ChainState, config: RunnableConfig) -> str:\n",
    "    question = state['messages'][0].content\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    prompt_str = \"\"\"\n",
    "    Give only the final answer with its unit where relevant without re-stating the question.\n",
    "    \\nResponse:\\n{messages}\\n\n",
    "    Question:\\n{question}\\n\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"messages\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'question':question,'messages':last_message})\n",
    "\n",
    "    return {\"response\":response, \"messages\": [response],\"ground_truth\":ground_truth}\n",
    "\n",
    "class ChainState(TypedDict):\n",
    "    \"\"\"LangGraph state with enhanced scoring.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    ground_truth: str\n",
    "    score_reasoning: str\n",
    "    detailed_score: dict\n",
    "    consensus_score: bool\n",
    "    response: str\n",
    "\n",
    "def advanced_scorer(state: ChainState, config: RunnableConfig) -> str:\n",
    "    \"\"\"Advanced scoring agent that considers multiple aspects of the answer.\"\"\"\n",
    "    \n",
    "    prompt_str = \"\"\"\n",
    "    Perform a detailed analysis of the model's answer compared to the ground truth.\n",
    "    Consider multiple aspects in your evaluation:\n",
    "    \n",
    "    1. Numerical Accuracy:\n",
    "       - Extract and compare numerical values\n",
    "       - Consider acceptable margin of error (±1% or correct to the whole number)\n",
    "       - Check for unit consistency\n",
    "    \n",
    "    2. Conceptual Correctness:\n",
    "       - Verify if the approach/methodology is correct\n",
    "       - Check if all required components are present\n",
    "    \n",
    "    3. Context Relevance:\n",
    "       - Ensure the answer addresses the specific question\n",
    "       - Verify if any contextual requirements are met\n",
    "    \n",
    "    Question: {query}\n",
    "    Ground truth: {ground_truth}\n",
    "    Model answer: {answer}\n",
    "    \n",
    "    Analyze the response and return a JSON object with this exact structure:\n",
    "    {{\n",
    "        \"numerical_accuracy\": {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<explanation>\"\n",
    "        }},\n",
    "        \"conceptual_correctness\": {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<explanation>\"\n",
    "        }},\n",
    "        \"context_relevance\": {{\n",
    "            \"score\": <float between 0 and 1>,\n",
    "            \"reasoning\": \"<explanation>\"\n",
    "        }},\n",
    "        \"overall_score\": <float between 0 and 1>,\n",
    "        \"is_correct\": <boolean>\n",
    "    }}\n",
    "    \n",
    "    Ensure your response is a valid JSON object matching this structure exactly.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"query\", \"answer\", \"ground_truth\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | JsonOutputParser()\n",
    "\n",
    "    detailed_score = chain.invoke({\n",
    "        'query': state['messages'][0].content,\n",
    "        'answer': state['response'],\n",
    "        'ground_truth': state['ground_truth']\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"response\": state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\": state['ground_truth'],\n",
    "        \"detailed_score\": detailed_score\n",
    "    }\n",
    "\n",
    "def consensus_scorer(state: ChainState, config: RunnableConfig) -> str:\n",
    "    \"\"\"Combine original score with advanced score for final decision.\"\"\"\n",
    "    \n",
    "    prompt_str = \"\"\"\n",
    "    Analyze both scoring approaches and provide a final consensus.\n",
    "    \n",
    "    Original scoring reasoning:\n",
    "    {original_score}\n",
    "    \n",
    "    Detailed scoring analysis:\n",
    "    {detailed_score}\n",
    "    \n",
    "    Based on both scoring methods, make a final decision.\n",
    "    Consider:\n",
    "    1. The original binary score\n",
    "    2. The detailed numerical accuracy score\n",
    "    3. The conceptual correctness score\n",
    "    4. The context relevance score\n",
    "    \n",
    "    Provide your response in exactly this format:\n",
    "    DECISION: [True/False]\n",
    "    REASONING: [Your brief explanation]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_str,\n",
    "        input_variables=[\"original_score\", \"detailed_score\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    consensus = chain.invoke({\n",
    "        'original_score': state['score_reasoning'],\n",
    "        'detailed_score': str(state['detailed_score'])\n",
    "    })\n",
    "\n",
    "    final_decision = consensus.split('\\n')[0].replace('DECISION:', '').strip() == 'True'\n",
    "\n",
    "    return {\n",
    "        \"response\": state['response'],\n",
    "        \"messages\": state['messages'],\n",
    "        \"ground_truth\": state['ground_truth'],\n",
    "        \"score_reasoning\": state['score_reasoning'],\n",
    "        \"detailed_score\": state['detailed_score'],\n",
    "        \"consensus_score\": final_decision\n",
    "    }\n",
    "\n",
    "# Update the graph with new scoring nodes\n",
    "graph_builder = StateGraph(ChainState)\n",
    "graph_builder.add_node(\"call_tool\", call_chain)\n",
    "graph_builder.add_node(\"execute_tool\", ToolNode(tools))\n",
    "graph_builder.add_node(\"call_model\", call_model)\n",
    "graph_builder.add_node(\"clean_response\", clean_response)\n",
    "graph_builder.add_node(\"llm_score\", llm_score)\n",
    "graph_builder.add_node(\"advanced_scorer\", advanced_scorer)\n",
    "graph_builder.add_node(\"consensus_scorer\", consensus_scorer)\n",
    "graph_builder.set_entry_point(\"call_tool\")\n",
    "\n",
    "# Define the enhanced flow\n",
    "graph_builder.add_edge(\"call_tool\", \"execute_tool\")\n",
    "graph_builder.add_edge(\"execute_tool\", \"call_model\")\n",
    "graph_builder.add_edge(\"call_model\", \"clean_response\")\n",
    "graph_builder.add_edge(\"clean_response\", \"llm_score\")\n",
    "graph_builder.add_edge(\"llm_score\", \"advanced_scorer\")\n",
    "graph_builder.add_edge(\"advanced_scorer\", \"consensus_scorer\")\n",
    "graph_builder.add_edge(\"consensus_scorer\", END)\n",
    "\n",
    "chain = graph_builder.compile()\n",
    "\n",
    "def qa_maths_reasoning_langgraph_advanced_scorer(text: str, question: str, ground_truth):\n",
    "    query = f\"\"\"read the following text:\\n---{text}\\n---\\nQuestion: {question}\"\"\"\n",
    "    result = chain.invoke({'messages': ['user', query], 'ground_truth': ground_truth})\n",
    "    result['question'] = question\n",
    "    result['overall_score'] = result['detailed_score']['overall_score']\n",
    "    return result\n",
    "\n",
    "# display(Image(chain.get_graph().draw_mermaid_png()))\n",
    "# result = qa_maths_reasoning_langgraph(text, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliasMelas\\Documents\\GitCode\\venv_master\\.venv_chatbot\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to summarize the calculation of the change in the effects of foreign operations from 2002 to 2004, which resulted in a percentage change of approximately -76.8%. \n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The effects of foreign operations changed by approximately -76.8% from 2002 to 2004.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "call_model response: content='user' additional_kwargs={} response_metadata={} id='39c4756b-2341-4e8e-90dc-099a78bacf28'\n"
     ]
    }
   ],
   "source": [
    "reload(qa_advanced_scorer)\n",
    "\n",
    "text = \"\"\"\n",
    "gain or loss on ownership change in map results from contributions to map of certain environmental capital expenditures and leased property acquisitions funded by marathon and ashland .\n",
    "in accordance with map 2019s limited liability company agreement , in certain instances , environmental capital expenditures and acquisitions of leased properties are funded by the original contributor of the assets , but no change in ownership interest may result from these contributions .\n",
    "an excess of ashland funded improvements over marathon funded improvements results in a net gain and an excess of marathon funded improvements over ashland funded improvements results in a net loss .\n",
    "cost of revenues increased by $ 5.822 billion in 2004 from 2003 and by $ 6.040 billion in 2003 from 2002 .\n",
    "the increases are primarily in the rm&t segment and result from higher acquisition costs for crude oil , refined products , refinery charge and blend feedstocks and increased manufacturing expenses .\n",
    "selling , general and administrative expenses increased by $ 105 million in 2004 from 2003 and by $ 97 million in 2003 from 2002 .\n",
    "the increase in 2004 was primarily due to increased stock-based compensation and higher costs associated with business transformation and outsourcing .\n",
    "our 2004 results were also impacted by start-up costs associated with the lng project in equatorial guinea and the increased cost of complying with governmental regulations .\n",
    "the increase in 2003 was primarily due to increased employee benefit expenses ( caused by increased pension expense resulting from changes in actuarial assumptions and a decrease in realized returns on plan assets ) and other employee related costs .\n",
    "additionally , during 2003 , we recorded a charge of $ 24 million related to organizational and business process changes .\n",
    "inventory market valuation reserve ( 2018 2018imv 2019 2019 ) is established to reduce the cost basis of inventories to current market value .\n",
    "generally , we will establish an imv reserve when crude oil prices fall below $ 22 per barrel .\n",
    "the 2002 results of operations include credits to income from operations of $ 71 million , reversing the imv reserve at december 31 , 2001 .\n",
    "net interest and other financial costs decreased by $ 25 million in 2004 from 2003 and by $ 82 million in 2003 from 2002 .\n",
    "the decrease in 2004 is primarily due to an increase in interest income .\n",
    "the decrease in 2003 is primarily due to an increase in capitalized interest related to increased long-term construction projects , the favorable effect of interest rate swaps , the favorable effect of a reduction in interest on tax deficiencies and increased interest income on investments .\n",
    "additionally , included in net interest and other financing costs are foreign currency gains of $ 9 million , $ 13 million and $ 8 million for 2004 , 2003 and 2002 .\n",
    "loss from early extinguishment of debt in 2002 was attributable to the retirement of $ 337 million aggregate principal amount of debt , resulting in a loss of $ 53 million .\n",
    "minority interest in income of map , which represents ashland 2019s 38 percent ownership interest , increased by $ 230 million in 2004 from 2003 and by $ 129 million in 2003 from 2002 .\n",
    "map income was higher in 2004 compared to 2003 and in 2003 compared to 2002 as discussed below in the rm&t segment .\n",
    "minority interest in loss of equatorial guinea lng holdings limited , which represents gepetrol 2019s 25 percent ownership interest , was $ 7 million in 2004 , primarily resulting from gepetrol 2019s share of start-up costs associated with the lng project in equatorial guinea .\n",
    "provision for income taxes increased by $ 143 million in 2004 from 2003 and by $ 215 million in 2003 from 2002 , primarily due to $ 388 million and $ 720 million increases in income before income taxes .\n",
    "the effective tax rate for 2004 was 36.6 percent compared to 36.6 percent and 42.1 percent for 2003 and 2002 .\n",
    "the higher rate in 2002 was due to the united kingdom enactment of a supplementary 10 percent tax on profits from the north sea oil and gas production , retroactively effective to april 17 , 2002 .\n",
    "in 2002 , we recognized a one-time noncash deferred tax adjustment of $ 61 million as a result of the rate increase .\n",
    "the following is an analysis of the effective tax rate for the periods presented: .\n",
    "\n",
    "table:\n",
    "\n",
    "[['', '2004', '2003', '2002'], ['statutory tax rate', '35.0% ( 35.0 % )', '35.0% ( 35.0 % )', '35.0% ( 35.0 % )'], ['effects of foreign operations ( a )', '1.3', '-0.4 ( 0.4 )', '5.6'], ['state and local income taxes after federal income tax effects', '1.6', '2.2', '3.9'], ['other federal tax effects', '-1.3 ( 1.3 )', '-0.2 ( 0.2 )', '-2.4 ( 2.4 )'], ['effective tax rate', '36.6% ( 36.6 % )', '36.6% ( 36.6 % )', '42.1% ( 42.1 % )']]\n",
    "\n",
    "( a ) the deferred tax effect related to the enactment of a supplemental tax in the u.k .\n",
    "increased the effective tax rate 7.0 percent in .\n",
    "\"\"\"\n",
    "question = \"by what percent did effects of foreign operations decrease from 2002 to 2004?\"\n",
    "\n",
    "ground_truth = \"-76.8%\"\n",
    "\n",
    "# result = qa_maths_reasoning_langgraph(text, question, ground_truth)\n",
    "result = qa_advanced_scorer.qa_maths_reasoning_langgraph_advanced_scorer(text, question, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='user', additional_kwargs={}, response_metadata={}, id='39c4756b-2341-4e8e-90dc-099a78bacf28'),\n",
       "  HumanMessage(content=\"read the following text:\\n---\\ngain or loss on ownership change in map results from contributions to map of certain environmental capital expenditures and leased property acquisitions funded by marathon and ashland .\\nin accordance with map 2019s limited liability company agreement , in certain instances , environmental capital expenditures and acquisitions of leased properties are funded by the original contributor of the assets , but no change in ownership interest may result from these contributions .\\nan excess of ashland funded improvements over marathon funded improvements results in a net gain and an excess of marathon funded improvements over ashland funded improvements results in a net loss .\\ncost of revenues increased by $ 5.822 billion in 2004 from 2003 and by $ 6.040 billion in 2003 from 2002 .\\nthe increases are primarily in the rm&t segment and result from higher acquisition costs for crude oil , refined products , refinery charge and blend feedstocks and increased manufacturing expenses .\\nselling , general and administrative expenses increased by $ 105 million in 2004 from 2003 and by $ 97 million in 2003 from 2002 .\\nthe increase in 2004 was primarily due to increased stock-based compensation and higher costs associated with business transformation and outsourcing .\\nour 2004 results were also impacted by start-up costs associated with the lng project in equatorial guinea and the increased cost of complying with governmental regulations .\\nthe increase in 2003 was primarily due to increased employee benefit expenses ( caused by increased pension expense resulting from changes in actuarial assumptions and a decrease in realized returns on plan assets ) and other employee related costs .\\nadditionally , during 2003 , we recorded a charge of $ 24 million related to organizational and business process changes .\\ninventory market valuation reserve ( 2018 2018imv 2019 2019 ) is established to reduce the cost basis of inventories to current market value .\\ngenerally , we will establish an imv reserve when crude oil prices fall below $ 22 per barrel .\\nthe 2002 results of operations include credits to income from operations of $ 71 million , reversing the imv reserve at december 31 , 2001 .\\nnet interest and other financial costs decreased by $ 25 million in 2004 from 2003 and by $ 82 million in 2003 from 2002 .\\nthe decrease in 2004 is primarily due to an increase in interest income .\\nthe decrease in 2003 is primarily due to an increase in capitalized interest related to increased long-term construction projects , the favorable effect of interest rate swaps , the favorable effect of a reduction in interest on tax deficiencies and increased interest income on investments .\\nadditionally , included in net interest and other financing costs are foreign currency gains of $ 9 million , $ 13 million and $ 8 million for 2004 , 2003 and 2002 .\\nloss from early extinguishment of debt in 2002 was attributable to the retirement of $ 337 million aggregate principal amount of debt , resulting in a loss of $ 53 million .\\nminority interest in income of map , which represents ashland 2019s 38 percent ownership interest , increased by $ 230 million in 2004 from 2003 and by $ 129 million in 2003 from 2002 .\\nmap income was higher in 2004 compared to 2003 and in 2003 compared to 2002 as discussed below in the rm&t segment .\\nminority interest in loss of equatorial guinea lng holdings limited , which represents gepetrol 2019s 25 percent ownership interest , was $ 7 million in 2004 , primarily resulting from gepetrol 2019s share of start-up costs associated with the lng project in equatorial guinea .\\nprovision for income taxes increased by $ 143 million in 2004 from 2003 and by $ 215 million in 2003 from 2002 , primarily due to $ 388 million and $ 720 million increases in income before income taxes .\\nthe effective tax rate for 2004 was 36.6 percent compared to 36.6 percent and 42.1 percent for 2003 and 2002 .\\nthe higher rate in 2002 was due to the united kingdom enactment of a supplementary 10 percent tax on profits from the north sea oil and gas production , retroactively effective to april 17 , 2002 .\\nin 2002 , we recognized a one-time noncash deferred tax adjustment of $ 61 million as a result of the rate increase .\\nthe following is an analysis of the effective tax rate for the periods presented: .\\n\\ntable:\\n\\n[['', '2004', '2003', '2002'], ['statutory tax rate', '35.0% ( 35.0 % )', '35.0% ( 35.0 % )', '35.0% ( 35.0 % )'], ['effects of foreign operations ( a )', '1.3', '-0.4 ( 0.4 )', '5.6'], ['state and local income taxes after federal income tax effects', '1.6', '2.2', '3.9'], ['other federal tax effects', '-1.3 ( 1.3 )', '-0.2 ( 0.2 )', '-2.4 ( 2.4 )'], ['effective tax rate', '36.6% ( 36.6 % )', '36.6% ( 36.6 % )', '42.1% ( 42.1 % )']]\\n\\n( a ) the deferred tax effect related to the enactment of a supplemental tax in the u.k .\\nincreased the effective tax rate 7.0 percent in .\\n\\n---\\nQuestion: by what percent did effects of foreign operations change from 2002 to 2004?\", additional_kwargs={}, response_metadata={}, id='238887e2-87ea-4316-bd58-b7d95874be98'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IFg7yVx0b02lILSAC6BQsYU3', 'function': {'arguments': '{\"expression\": \"1.3 - 5.6\"}', 'name': 'calculator'}, 'type': 'function'}, {'id': 'call_c7CI88zpkhGn5HtscyNQk7tZ', 'function': {'arguments': '{\"expression\": \"(1.3 - 5.6) / 5.6 * 100\"}', 'name': 'calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 1309, 'total_tokens': 1377, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_d54531d9eb', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-7314e2ef-9ddb-4f02-9ded-747eb0e5fc40-0', tool_calls=[{'name': 'calculator', 'args': {'expression': '1.3 - 5.6'}, 'id': 'call_IFg7yVx0b02lILSAC6BQsYU3', 'type': 'tool_call'}, {'name': 'calculator', 'args': {'expression': '(1.3 - 5.6) / 5.6 * 100'}, 'id': 'call_c7CI88zpkhGn5HtscyNQk7tZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1309, 'output_tokens': 68, 'total_tokens': 1377}),\n",
       "  ToolMessage(content='-4.3', name='calculator', id='e51553ef-4be4-4dc7-b4d8-70b18247b7bf', tool_call_id='call_IFg7yVx0b02lILSAC6BQsYU3'),\n",
       "  ToolMessage(content='-76.78571428571429', name='calculator', id='0d959cde-2683-4b1b-9746-0f84844b264c', tool_call_id='call_c7CI88zpkhGn5HtscyNQk7tZ'),\n",
       "  HumanMessage(content='-76.79', additional_kwargs={}, response_metadata={}, id='2ce6f025-929b-4f10-b6cf-95fa3b69153f')],\n",
       " 'ground_truth': '-76.8%',\n",
       " 'score_reasoning': \"Let's solve this step by step:\\n\\n1. Ground truth number: The numerical value extracted from the ground truth is -76.8.\\n2. Model answer number: The numerical value extracted from the model answer is -76.79.\\n\\nNow, we will compare these two values within a reasonable margin. A common tolerance for numerical comparisons is 0.01 (1%).\\n\\n- The absolute difference between -76.8 and -76.79 is:\\n  \\\\[\\n  |-76.8 - (-76.79)| = |-76.8 + 76.79| = |0.01| = 0.01\\n  \\\\]\\n\\nSince the absolute difference (0.01) is equal to the tolerance (0.01), we consider them equivalent.\\n\\nFinal answer (True/False): True\",\n",
       " 'detailed_score': {'numerical_accuracy': {'score': 0.95,\n",
       "   'reasoning': \"The model's answer of -76.79% is within ±1% of the ground truth of -76.8%. The absolute difference is 0.01%, which is acceptable. Both values are negative, indicating they are in the same context of decline.\"},\n",
       "  'conceptual_correctness': {'score': 1.0,\n",
       "   'reasoning': \"The methodology used to derive the model's answer appears to be correct, as it closely matches the ground truth. All required components for numerical representation are present.\"},\n",
       "  'context_relevance': {'score': 1.0,\n",
       "   'reasoning': \"The model's answer directly addresses the question by providing a numerical value that is relevant to the context of the inquiry. There are no contextual requirements unmet.\"},\n",
       "  'overall_score': 0.98,\n",
       "  'is_correct': True},\n",
       " 'response': '-76.79',\n",
       " 'question': 'by what percent did effects of foreign operations change from 2002 to 2004?',\n",
       " 'overall_score': 0.98}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_data(data, verbose=False, convert_to_df=True, advanced=True):\n",
    "    result_list = []\n",
    "    for i,r in data.iterrows():\n",
    "\n",
    "        text =r['source_text']\n",
    "        qa = r['qa_exploded']\n",
    "\n",
    "        if str(qa)!='nan':\n",
    "            result = {}\n",
    "            question = qa['question']\n",
    "            answer = qa['answer']\n",
    "            if advanced:\n",
    "                response = qa_advanced_scorer.qa_maths_reasoning_langgraph_advanced_scorer(text,question, answer)\n",
    "                answer_llm = response['response']\n",
    "                score = response['detailed_score']['overall_score']\n",
    "            else:\n",
    "                response = qa_basic_scorer.qa_maths_reasoning_langgraph(text,question, answer)#[-1].content\n",
    "                answer_llm = response['response']\n",
    "                display(response)\n",
    "                score = response['score']\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Question: {question}')\n",
    "                print(f'Answer (Ground Truth): {answer}')\n",
    "                print(f'Answer (LLM): {answer_llm}')\n",
    "                print(f'Score: {score}')\n",
    "                print('\\n')\n",
    "\n",
    "            result_list.append(response)\n",
    "\n",
    "        else:\n",
    "            print('no qa')\n",
    "\n",
    "    if convert_to_df:\n",
    "        return pd.DataFrame(result_list)\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# df_scores = score_data(data_clean[:1], advanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_row(row, verbose=False, advanced=False):\n",
    "    text = row['source_text']\n",
    "    qa = row['qa_exploded']\n",
    "\n",
    "    if str(qa) != 'nan':\n",
    "        question = qa['question']\n",
    "        answer = qa['answer']\n",
    "        if advanced:\n",
    "            response = qa_advanced_scorer.qa_maths_reasoning_langgraph_advanced_scorer(text, question, answer)\n",
    "            answer_llm = response['response']\n",
    "            score = response['detailed_score']['overall_score']\n",
    "        else:\n",
    "            response = qa_basic_scorer.qa_maths_reasoning_langgraph(text, question, answer)\n",
    "            answer_llm = response['response']\n",
    "            score = response['score']\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Question: {question}')\n",
    "            print(f'Answer (Ground Truth): {answer}')\n",
    "            print(f'Answer (LLM): {answer_llm}')\n",
    "            print(f'Score: {score}')\n",
    "            print('\\n')\n",
    "\n",
    "        return response\n",
    "    else:\n",
    "        print('no qa')\n",
    "        return None\n",
    "\n",
    "def score_data_concurrently(data, verbose=False, convert_to_df=True, max_workers=5, advanced=False):\n",
    "    result_list = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_row = {executor.submit(process_row, row, verbose,advanced=advanced): i for i, row in data.iterrows()}\n",
    "\n",
    "        for future in as_completed(future_to_row):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                result_list.append(result)\n",
    "\n",
    "    if convert_to_df:\n",
    "        return pd.DataFrame(result_list)\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# df_scores = score_data_concurrently(data_clean[:5],advanced_scorer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores: {'accuracy': 0.6229333333333333}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def process_sample(sample_df):\n",
    "    # Calculate F1 score for the current sample based on 'score' column\n",
    "    y_true = sample_df['overall_score']\n",
    "    y_pred = [1] * len(y_true)  # assuming `True` indicates correct predictions by the model\n",
    "\n",
    "    # Calculate multiple metrics\n",
    "    metrics = {\n",
    "        'accuracy': np.mean(y_true),\n",
    "    }\n",
    "    return metrics, sample_df\n",
    "\n",
    "def evaluate_samples(data, k, n, max_workers=5, advanced=False):\n",
    "    metrics_list = []\n",
    "    sample_dfs = []\n",
    "\n",
    "    def get_random_sample():\n",
    "        # Randomly sample `n` rows\n",
    "        # df_scores = score_data_concurrently(data.sample(n=n),advanced=advanced)\n",
    "        df_scores = score_data(data.sample(n=n), advanced=advanced)\n",
    "        # display(df_scores)\n",
    "        # df_scores['score'] = df_scores['score'].apply(lambda x: 1 if str(x)=='True' else 0)\n",
    "        # display(df_scores)\n",
    "        return df_scores\n",
    "\n",
    "    # Use ThreadPoolExecutor to calculate F1 scores concurrently\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create a list of future tasks for each sample\n",
    "        futures = [executor.submit(process_sample, get_random_sample()) for _ in range(k)]\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            metrics, sample_df = future.result()\n",
    "            metrics_list.append(metrics)\n",
    "            sample_dfs.append(sample_df)\n",
    "\n",
    "    # Aggregate results by averaging each metric across all samples\n",
    "    aggregated_metrics = {\n",
    "        'accuracy': np.mean([metrics['accuracy'] for metrics in metrics_list]),\n",
    "    }\n",
    "\n",
    "    concatenated_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "\n",
    "    return aggregated_metrics, concatenated_df\n",
    "\n",
    "# Example usage:\n",
    "k = 10 # Number of random samples to test\n",
    "n = 5   # Number of rows in each sample\n",
    "metrics, df_scores = evaluate_samples(data_clean, k, n, advanced=True)\n",
    "print(\"F1 Scores:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      0.622933\n",
       "std       0.452775\n",
       "min       0.000000\n",
       "25%       0.041667\n",
       "50%       0.966667\n",
       "75%       0.990000\n",
       "max       1.000000\n",
       "Name: overall_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6229333333333333}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_scores['overall_score'].describe(), metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.95,\n",
       "  'reasoning': \"The model's answer of -18.64% is very close to the ground truth of -19%. The difference is 0.36%, which is within the acceptable margin of error of ±1%. Both values are expressed as percentages, ensuring unit consistency.\"},\n",
       " 'conceptual_correctness': {'score': 1.0,\n",
       "  'reasoning': \"The methodology used to arrive at the model's answer appears to be correct, as it provides a numerical value that is relevant to the question. All required components for a percentage calculation are present.\"},\n",
       " 'context_relevance': {'score': 1.0,\n",
       "  'reasoning': \"The model's answer directly addresses the question by providing a percentage that is relevant to the context. There are no contextual requirements that are unmet.\"},\n",
       " 'overall_score': 0.98,\n",
       " 'is_correct': False}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores.iloc[0]['detailed_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: 4.5\n",
      "Question: what is the amount of cash raised from the issuance of shares during 2015 , in millions?\n",
      "Response: No data available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not provide any numerical value, which is required to compare against the ground truth of 4.5. Therefore, it fails to meet any criteria for numerical accuracy.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's response of 'No data available' indicates a lack of engagement with the question. It does not attempt to provide a numerical answer or any reasoning that would demonstrate an understanding of the question's requirements.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question posed, which requires a numerical response. The phrase 'No data available' is irrelevant to the context of providing a numerical value.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 12.8%\n",
      "Question: what is the percent of the purchased loans accounted for under the level-yield method included in the carrying amount of loan receivable net of purchased loans accounted for under the under the cost-recovery method\n",
      "Response: 26.44%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of 26.44% is significantly different from the ground truth of 12.8%. The difference exceeds the acceptable margin of error of ±1%, indicating a complete lack of numerical accuracy.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The methodology used to arrive at the model's answer is not clear, and it does not align with the expected approach to derive the ground truth value. Key components necessary for accurate calculation are missing.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question posed, as it provides a value that is unrelated to the ground truth. There are no contextual requirements met in the response.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 190\n",
      "Question: in shares in thousands , for the non-vested incentive/ performance unit shares , what was the change in balance between december 31 2013 and december 31 2014?\n",
      "Response: 190,000 shares\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of 190,000 shares is significantly different from the ground truth of 190. The difference is more than 1% and the units are inconsistent, as the ground truth is a simple number while the model's answer includes a unit of shares.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not correctly represent the numerical value requested. The methodology of providing a numerical answer is correct, but the actual value provided is incorrect and does not align with the ground truth.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question as it provides an answer that is not relevant to the ground truth. The context of the question requires a specific numerical value, which the model fails to provide.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 23\n",
      "Question: what was the net adjustments as recorded in 2011 in millions\n",
      "Response: $44 million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of $44 million is numerically incorrect when compared to the ground truth of 23. There is no acceptable margin of error, as the values are vastly different and in different units (dollars vs. a simple number).\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not follow the expected methodology or approach to arrive at the ground truth. It fails to provide any relevant calculations or reasoning that would lead to the number 23.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question posed by the user, as it provides a completely unrelated numerical value. There are no contextual requirements met.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 1611908200\n",
      "Question: what was the total value of the 1745 broadway property as of april 2007 based on the acquisition price?\n",
      "Response: $520,000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of $520,000 does not match the ground truth of 1611908200. There is a significant discrepancy between the two values, with no acceptable margin of error.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not reflect an understanding of the question or the required numerical output. The methodology appears incorrect as it does not provide a relevant or accurate calculation.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question posed, as it provides a monetary value instead of the expected numerical value of 1611908200. There is no contextual alignment with the ground truth.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: .44%\n",
      "Question: percent change of average shares outstanding when taking dilution into consideration in 2008?\n",
      "Response: -3.87%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of -3.87% is significantly different from the ground truth of 0.44%. The absolute difference is 4.31%, which exceeds the acceptable margin of error of ±1%. Additionally, the signs of the values are inconsistent, indicating a fundamental error in the numerical representation.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not reflect a correct understanding of the problem, as it provides a negative percentage when the ground truth is a positive percentage. This indicates a misunderstanding of the underlying concept or calculation required.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question posed, as it provides an irrelevant and incorrect numerical value that does not relate to the ground truth. The context of the question is not met.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 557563\n",
      "Question: what is the total of credit-related financial instruments in 2006? ( $ )\n",
      "Response: $557,563 million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of $557,563 million is incorrect as it represents a value of 557,563,000,000, which is significantly larger than the ground truth of 557,563. There is no acceptable margin of error since the values are not even close.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's methodology is flawed as it misinterprets the numerical value by adding an incorrect unit of 'million'. The required components of the answer are not present, as it fails to represent the ground truth accurately.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': 'The answer does not address the specific question correctly, as it provides a value that is not relevant to the ground truth. The contextual requirement of matching the numerical value is not met.'},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 91.5%\n",
      "Question: what was the percent of the increase in the operating income from 2010 to 2011\n",
      "Response: 10%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of 10% is significantly different from the ground truth of 91.5%. The difference exceeds the acceptable margin of error, and the units are consistent as both are percentages.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not reflect an understanding of the problem, as it provides a value that is not only incorrect but also lacks any relevant methodology or reasoning to arrive at that answer.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question posed, as it provides a completely unrelated percentage that does not align with the context of the ground truth.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: -18.8%\n",
      "Question: what was the percent of the decline in the weighted average risk-free interest rate from 2002 to 2003\n",
      "Response: 18.77%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of 18.77% is not only incorrect in sign but also significantly different in magnitude from the ground truth of -18.8%. The absolute difference is 37.57%, which exceeds any acceptable margin of error.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not reflect the correct approach or methodology, as it fails to recognize the negative value indicated in the ground truth. The absence of negative sign indicates a fundamental misunderstanding of the problem.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question as it provides a positive percentage instead of the required negative percentage. This indicates a lack of relevance to the context of the question.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 19%\n",
      "Question: what was the percent change in revenue recognized under the agreement between 2004and 2005?\n",
      "Response: 108.57%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of 108.57% is significantly different from the ground truth of 19%. The difference exceeds the acceptable margin of error, and the units are consistent as both are percentages.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The methodology used to arrive at the model's answer is incorrect, as it does not align with the expected calculation or reasoning that would yield the ground truth of 19%.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question, as it provides a value that is not relevant to the ground truth provided.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: 385\n",
      "Question: if the tax controversy from softer is resolved favorably , what would the gross assets acquired be , in millions?\n",
      "Response: $431 million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of $431 million is significantly different from the ground truth of 385. The difference is substantial, exceeding any acceptable margin of error.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not reflect the correct numerical value or context, indicating a failure in understanding the question or the required methodology.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the specific question, as it provides a value that is unrelated to the ground truth of 385.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: yes\n",
      "Question: did the company increase it's quarterly dividend rate from 2007 to 2008?\n",
      "Response: The company did not increase its quarterly dividend rate; it decreased it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer indicates a decrease in the quarterly dividend rate, while the ground truth simply states 'yes', which implies an increase. There is a complete mismatch in the numerical interpretation.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's methodology is incorrect as it misinterprets the question. The question likely seeks a confirmation of an increase, and the model's response does not address this correctly.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the user's question, which is a simple affirmative. Instead, it provides information about a decrease, which is irrelevant to the context of the question.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n",
      "Ground Truth: yes\n",
      "Question: was the dividend declared on february 10 , 2015 greater than the quarterly cash dividend on our common stock declared on february 12 2014?\n",
      "Response: $0.025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'numerical_accuracy': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer of $0.025 does not match the ground truth of 'yes'. There is no numerical correlation between the two, and the model's answer is not a numerical representation of 'yes'.\"},\n",
       " 'conceptual_correctness': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer does not address the question correctly. The ground truth is a simple affirmative response ('yes'), while the model provided a numerical value, indicating a misunderstanding of the question's requirements.\"},\n",
       " 'context_relevance': {'score': 0.0,\n",
       "  'reasoning': \"The model's answer is not relevant to the context of the question. The question likely required a binary response, and the model's numerical output does not fulfill this requirement.\"},\n",
       " 'overall_score': 0.0,\n",
       " 'is_correct': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for i,r in df_scores.loc[df_scores['overall_score']==0].iterrows():\n",
    "    ground_truth = r['ground_truth']\n",
    "    question = r['question']\n",
    "    # score_reasoning = r['score_reasoning']\n",
    "    detailed_score = r['detailed_score']\n",
    "    score = r['overall_score']\n",
    "    response = r['response']\n",
    "    print(f'Ground Truth: {ground_truth}')\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Response: {response}')\n",
    "    # print(f'Score reasoning: {score_reasoning}')\n",
    "    display(detailed_score)\n",
    "    print(f'Score: {score}')\n",
    "    print('\\n-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"third-party sales for the engineered products and solutions segment improved 7% ( 7 % ) in 2016 compared with 2015 , primarily attributable to higher third-party sales of the two acquired businesses ( $ 457 ) , primarily related to the aerospace end market , and increased demand from the industrial gas turbine end market , partially offset by lower volumes in the oil and gas end market and commercial transportation end market as well as pricing pressures in aerospace .\\nthird-party sales for this segment improved 27% ( 27 % ) in 2015 compared with 2014 , largely attributable to the third-party sales ( $ 1310 ) of the three acquired businesses ( see above ) , and higher volumes in this segment 2019s legacy businesses , both of which were primarily related to the aerospace end market .\\nthese positive impacts were slightly offset by unfavorable foreign currency movements , principally driven by a weaker euro .\\natoi for the engineered products and solutions segment increased $ 47 , or 8% ( 8 % ) , in 2016 compared with 2015 , primarily related to net productivity improvements across all businesses as well as the volume increase from both the rti acquisition and organic revenue growth , partially offset by a lower margin product mix and pricing pressures in the aerospace end market .\\natoi for this segment increased $ 16 , or 3% ( 3 % ) , in 2015 compared with 2014 , principally the result of net productivity improvements across most businesses , a positive contribution from acquisitions , and overall higher volumes in this segment 2019s legacy businesses .\\nthese positive impacts were partially offset by unfavorable price and product mix , higher costs related to growth projects , and net unfavorable foreign currency movements , primarily related to a weaker euro .\\nin 2017 , demand in the commercial aerospace end market is expected to remain strong , driven by the ramp up of new aerospace engine platforms , somewhat offset by continued customer destocking and engine ramp-up challenges .\\ndemand in the defense end market is expected to grow due to the continuing ramp-up of certain aerospace programs .\\nadditionally , net productivity improvements are anticipated while pricing pressure across all markets is likely to continue .\\ntransportation and construction solutions .\\n\\ntable:\\n\\n[['', '2016', '2015', '2014'], ['third-party sales', '$ 1802', '$ 1882', '$ 2021'], ['atoi', '$ 176', '$ 166', '$ 180']]\\n\\nthe transportation and construction solutions segment produces products that are used mostly in the nonresidential building and construction and commercial transportation end markets .\\nsuch products include integrated aluminum structural systems , architectural extrusions , and forged aluminum commercial vehicle wheels , which are sold both directly to customers and through distributors .\\na small part of this segment also produces aluminum products for the industrial products end market .\\ngenerally , the sales and costs and expenses of this segment are transacted in the local currency of the respective operations , which are primarily the u.s .\\ndollar , the euro , and the brazilian real .\\nthird-party sales for the transportation and construction solutions segment decreased 4% ( 4 % ) in 2016 compared with 2015 , primarily driven by lower demand from the north american commercial transportation end market , which was partially offset by rising demand from the building and construction end market .\\nthird-party sales for this segment decreased 7% ( 7 % ) in 2015 compared with 2014 , primarily driven by unfavorable foreign currency movements , principally caused by a weaker euro and brazilian real , and lower volume related to the building and construction end market , somewhat offset by higher volume related to the commercial transportation end market .\\natoi for the transportation and construction solutions segment increased $ 10 , or 6% ( 6 % ) , in 2016 compared with 2015 , principally driven by net productivity improvements across all businesses and growth in the building and construction segment , partially offset by lower demand in the north american heavy duty truck and brazilian markets. .\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.loc[data_clean['question']=='considering the years 2015-2016 , how bigger is the growth of the third-party sales for the engineered products and solutions segment in comparison with the transportation and construction solutions one?']['source_text'][316]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
